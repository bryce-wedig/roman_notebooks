{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Draft: Roman Science Platform Data Access in the Cloud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Imports](#Imports)\n",
    "* [Introduction](#Introduction)\n",
    "* [Enabling Cloud Access](#Enabling-Cloud-Access)\n",
    "* [Accessing MAST Data](#Accessing-MAST-Data)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports\n",
    "Here we import the required packages for our data access examples. Some unique packages include:\n",
    "- *astropy.io fits* for accessing FITS files\n",
    "- *astropy.mast Observations* for accessing, searching, and selecting data from other missions\n",
    "- *s3fs* for streaming in data directly from the cloud\n",
    "- *matplotlib.pyplot* for plotting data\n",
    "- *numpy* for easily setting the plotting bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "The first step to any project is to acquire the necessary data to analyse. This notebook is designed to provide examples for how to access data from the science platform. In particular, it demonstrates how to stream data from the cloud directly into memory, circumventing the need to download the data locally and use excess storage. This method of accessing the data on the cloud is HIGHLY recommended, however we understand that some use-cases will require the data to be downloaded locally so we show an example of how to do that at the end of the notebook.\n",
    "\n",
    "Here-in we examine how to download data from two classes of sources:\n",
    "- the STScI MAST server which hosts data for in-flight telescopes including Hubble, TESS, and JWST\n",
    "- simulated Roman Space Telescope data from the Troxel suite of simulations (see [Troxel 2023](https://ui.adsabs.harvard.edu/abs/2023MNRAS.522.2801T/abstract) for more details on the simulation methods used)\n",
    "\n",
    "\n",
    "### Defining terms\n",
    "- Cloud computing: the practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer\n",
    "- AWS: Amazon Web Services (AWS) is the cloud computing landscape that is provided by Amazon\n",
    "- URI: a Universal Resource Identifier (URI) is a sequence of characters that identify a name or a unique resource on the Internet. Website URLs are a subclass of URIs.\n",
    "- AWS S3: Amazon Simple Storage Service (S3) is a very scalable and inexpensive object storage service on the AWS cloud platform. The storage containers are referred to as \"buckets\" so we will often refer to these storage devices as \"S3 buckets\" or \"S3 servers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enabling Cloud Access\n",
    "The most important step for accessing data from the cloud is to enable *astroquery* to retreive URIs and other relevant cloud information. Even if we are working locally and plan to download the data files (not recommended for Roman data), we need to use this command to copy the file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using the S3 STScI public dataset [astroquery.mast.cloud]\n"
     ]
    }
   ],
   "source": [
    "Observations.enable_cloud_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing MAST Data\n",
    "In this section, we will go through the steps to retreive archived MAST data from the cloud including how to query the archive, stream the files directly from the cloud, as well as download them locally.\n",
    "\n",
    "### Querying MAST\n",
    "Now we are ready to begin our query. This example is rather simple, but is quick and easy to reproduce. We will be querying Hubble ACS/WFC data in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table masked=True length=4</i>\n",
       "<table id=\"table6509468160\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>obsID</th><th>obs_collection</th><th>dataproduct_type</th><th>obs_id</th><th>description</th><th>type</th><th>dataURI</th><th>productType</th><th>productGroupDescription</th><th>productSubGroupDescription</th><th>productDocumentationURL</th><th>project</th><th>prvversion</th><th>proposal_id</th><th>productFilename</th><th>size</th><th>parent_obsid</th><th>dataRights</th><th>calib_level</th><th>filters</th></tr></thead>\n",
       "<thead><tr><th>str8</th><th>str3</th><th>str5</th><th>str35</th><th>str64</th><th>str1</th><th>str67</th><th>str9</th><th>str28</th><th>str11</th><th>str1</th><th>str7</th><th>str20</th><th>str5</th><th>str50</th><th>int64</th><th>str8</th><th>str6</th><th>int64</th><th>str9</th></tr></thead>\n",
       "<tr><td>24832664</td><td>HST</td><td>image</td><td>jbeveo010</td><td>DADS DRZ file - Calibrated combined image ACS/WFC3/WFPC2/STIS</td><td>D</td><td>mast:HST/product/jbeveo010_drz.fits</td><td>SCIENCE</td><td>Minimum Recommended Products</td><td>DRZ</td><td>--</td><td>CALACS</td><td>DrizzlePac 3.6.2</td><td>12062</td><td>jbeveo010_drz.fits</td><td>219608640</td><td>26423318</td><td>PUBLIC</td><td>3</td><td>F606W</td></tr>\n",
       "<tr><td>24832664</td><td>HST</td><td>image</td><td>jbeveo010</td><td>DADS DRZ file - Calibrated combined image ACS/WFC3/WFPC2/STIS</td><td>D</td><td>mast:HST/product/jbeveo010_drz.fits</td><td>SCIENCE</td><td>Minimum Recommended Products</td><td>DRZ</td><td>--</td><td>CALACS</td><td>DrizzlePac 3.6.2</td><td>12062</td><td>jbeveo010_drz.fits</td><td>219608640</td><td>24832664</td><td>PUBLIC</td><td>3</td><td>F606W</td></tr>\n",
       "<tr><td>24832668</td><td>HST</td><td>image</td><td>jbevet010</td><td>DADS DRZ file - Calibrated combined image ACS/WFC3/WFPC2/STIS</td><td>D</td><td>mast:HST/product/jbevet010_drz.fits</td><td>SCIENCE</td><td>Minimum Recommended Products</td><td>DRZ</td><td>--</td><td>CALACS</td><td>DrizzlePac 3.6.2</td><td>12062</td><td>jbevet010_drz.fits</td><td>219608640</td><td>24832668</td><td>PUBLIC</td><td>3</td><td>F606W</td></tr>\n",
       "<tr><td>24832668</td><td>HST</td><td>image</td><td>jbevet010</td><td>DADS DRZ file - Calibrated combined image ACS/WFC3/WFPC2/STIS</td><td>D</td><td>mast:HST/product/jbevet010_drz.fits</td><td>SCIENCE</td><td>Minimum Recommended Products</td><td>DRZ</td><td>--</td><td>CALACS</td><td>DrizzlePac 3.6.2</td><td>12062</td><td>jbevet010_drz.fits</td><td>219608640</td><td>26421364</td><td>PUBLIC</td><td>3</td><td>F606W</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table masked=True length=4>\n",
       " obsID   obs_collection dataproduct_type   obs_id  ... parent_obsid dataRights calib_level filters\n",
       "  str8        str3            str5         str35   ...     str8        str6       int64      str9 \n",
       "-------- -------------- ---------------- --------- ... ------------ ---------- ----------- -------\n",
       "24832664            HST            image jbeveo010 ...     26423318     PUBLIC           3   F606W\n",
       "24832664            HST            image jbeveo010 ...     24832664     PUBLIC           3   F606W\n",
       "24832668            HST            image jbevet010 ...     24832668     PUBLIC           3   F606W\n",
       "24832668            HST            image jbevet010 ...     26421364     PUBLIC           3   F606W"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = Observations.query_criteria(obs_collection='HST',\n",
    "                                        filters='F606W',\n",
    "                                        instrument_name='ACS/WFC',\n",
    "                                        proposal_id=['12062'],\n",
    "                                        dataRights='PUBLIC')\n",
    "products = Observations.get_product_list(obs)\n",
    "filtered = Observations.filter_products(products,\n",
    "                                        productSubGroupDescription='DRZ')\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our query, we specify that we want to look at HST data using the F606W filter and ACS/WFC. We also specify the proposal id to easily get the data of interest. Once we get the desired observations, we gather the list of products that go into the obervations. We then filter the products to gather all the drizzled data products (as they have higher resolution and will look better with simple plotting) which leaves us with four filtered products.\n",
    "\n",
    "Now that we have our desired products, we can gather the URIs for each of the files which indicate the files' locations in the MAST AWS S3 servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 2 of 4 products were duplicates. Only downloading 2 unique product(s). [astroquery.mast.observations]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://stpubdata/hst/public/jbev/jbeveo010/jbeveo010_drz.fits',\n",
       " 's3://stpubdata/hst/public/jbev/jbevet010/jbevet010_drz.fits']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uris = Observations.get_cloud_uris(filtered)\n",
    "uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `get_cloud_uris` checks for duplicates in the provided products to reduce the data access volume. It is also important to not that `get_cloud_uris` will always return a list. Thus we will need to gather the indivual URI string to access the files. Let's choose the first URI for the remainder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = uris[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming files directly into memory\n",
    "Here, we will use `s3fs` to directly access the data stored in the AWS S3 servers. Note that we must set `anon=True` to acces the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can see that the URI points to a FITS file, we can use `astropy` to access the information in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: <class 's3fs.core.S3File'>\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU     846   ()      \n",
      "  1  SCI           1 ImageHDU        81   (4240, 4313)   float32   \n",
      "  2  WHT           1 ImageHDU        44   (4240, 4313)   float32   \n",
      "  3  CTX           1 ImageHDU        37   (4240, 4313)   int32   \n",
      "  4  HDRTAB        1 BinTableHDU    593   10R x 292C   [9A, 3A, K, D, D, D, D, D, D, D, D, D, D, D, D, D, K, 3A, 9A, 7A, 18A, 4A, D, D, D, D, 3A, D, D, D, D, D, D, D, D, D, D, D, D, K, 8A, 23A, D, D, D, D, K, K, K, 8A, K, 23A, 9A, 20A, K, 4A, K, D, K, K, K, K, 23A, D, D, D, D, K, K, 3A, 3A, 4A, 4A, L, D, D, D, 3A, 1A, K, D, D, D, D, D, 4A, 4A, 12A, 12A, 23A, 8A, 23A, 10A, 10A, D, D, 3A, 3A, 23A, 4A, 8A, 7A, 23A, D, K, D, 6A, 9A, 8A, D, D, L, 9A, 18A, 3A, K, 5A, 7A, 3A, D, 13A, 8A, 4A, 3A, L, K, L, K, L, K, K, D, D, D, D, D, D, 3A, 1A, D, 23A, D, D, D, 3A, 23A, L, 1A, 3A, 6A, D, 3A, 6A, K, D, D, D, D, D, D, D, D, D, D, 23A, D, D, D, D, 3A, D, D, D, 1A, K, K, K, K, K, K, 23A, K, 5A, 7A, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, 12A, D, 24A, 23A, D, 1A, 1A, D, K, D, D, 1A, 1A, D, 4A, K, D, K, 7A, D, D, D, D, D, 23A, 23A, D, 8A, D, 29A, D, 3A, D, L, D, D, 4A, 6A, 5A, 2A, D, 3A, K, 1A, 1A, 1A, 1A, D, D, D, D, D, D, 4A, D, 4A, D, 4A, K, 4A, 3A, 1A, L, K, K, 1A, D, D, D, D, K, 3A, L, L, 6A, L, D, D, 3A, D, D, 3A, 8A, 1A, D, K, D, L, 30A, L, 5A]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file in AWS: 'F' is the S3 file\n",
    "with fs.open(uri, 'rb') as f:\n",
    "    # Now actually read in the FITS file \n",
    "    with fits.open(f, 'readonly') as HDUlist:\n",
    "        HDUlist.info()\n",
    "        sci = HDUlist[1].data\n",
    "type(sci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming ASDF Files\n",
    "\n",
    "Though Roman data will eventually be available through MAST but in this testing phase, some simulated data have been placed in a separate S3 bucket. These files can be streamed in exactly the same way as the Hubble FITS file above. Additionally we can also peruse the available files similarly to a Unix terminal. A full list of commands can be found in the `s3fs` documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs = s3fs.S3FileSystem(anon=False, key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY, token=AWS_SESSION_TOKEN)\n",
    "roman_asdf_dir_uri = 's3://rdmt-sandbox-roman-wfi-l2/romanisim/'\n",
    "fs.ls(roman_asdf_dir_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see all the files available from romanisim simulations. Below we import `roman_datamodels` to read in the asdf file as an example. Please see the asdf data format notebook (____) for more information about accessing data within asdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roman_datamodels as rdm\n",
    "asdf_file_uri = roman_asdf_dir_uri + 'test.asdf'\n",
    "\n",
    "with fs.open(asdf_file_uri, 'rb') as f:\n",
    "    dm = rdm.open(f)\n",
    "    \n",
    "print(type(dm))\n",
    "print(dm.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Simulated Roman Data (to be implemented when data is available)\n",
    "\n",
    "Eventually, the Troxel 2023 data will be available in S3 buckets for use with the Roman Science Platform. When they are available, this section will discuss how to sort through and access those simulation files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Files Locally (not recommended)\n",
    "\n",
    "Though it is **not recommended**, there may be instances where data files must be downloaded locally for certain workflows. To do that, we can use the URIs and the `S3FileSystem.get` function (documentation [here](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem.get))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out as this use case is not recommended and should only be needed in rare circumstances\n",
    "#from pathlib import Path\n",
    "#local_file_path = Path('data/')\n",
    "#local_file_path.mkdir(parents=True, exist_ok=True)\n",
    "#fs.get(uri, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Resources\n",
    "Some additional information that may be helpful can be found at the following links:\n",
    "\n",
    "- [`s3fs` Documentation](https://s3fs.readthedocs.io/en/latest/api.html#)\n",
    "- [ASDF Data Format Notebook]\n",
    "- [Troxel 2023 Paper](https://academic.oup.com/mnras/article/522/2/2801/7076879?login=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this notebook\n",
    "The data streaming information from this notebook is built largely off of the TIKE data-acces notebook by Thomas Dutkiewicz.\n",
    "\n",
    "**Author:** Will Schultz  \n",
    "**Updated On:** 2024-05-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
